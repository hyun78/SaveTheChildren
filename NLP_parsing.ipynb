{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bagdongmin/anaconda2/lib/python2.7/site-packages/cffi/model.py:525: UserWarning: 'point_conversion_form_t' has no values explicitly defined; guessing that it is equivalent to 'unsigned int'\n",
      "  % self._get_c_name())\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import re\n",
    "from pattern.en import suggest\n",
    "filename = \"mlp_001.txt\"\n",
    "field = 'mylittlepony'\n",
    "filepath = 'data/{field}/{name}'.format(field=field,name=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#----------by DM-----------\n",
    "\n",
    "#input : word, output : word\n",
    "\n",
    "#---------------------\n",
    "\n",
    "\n",
    "\n",
    "#...제외 3개이상 알파벳 반복 -> 2개\n",
    "def three2two(word):\n",
    "    pattern = re.compile(r'([^\\.]+)\\1{2,}')\n",
    "    return pattern.sub(r'\\1\\1', word)\n",
    "\n",
    "# word -> word ('helllloooo' -> 'hello')\n",
    "def correct_lengthen(word):\n",
    "    short_word = three2two(word)\n",
    "    if short_word != word:\n",
    "        p_word = suggest(short_word) #(가장 유사한 단어, 확률) tuples\n",
    "        short_word = p_word[0][0] #제일 높은 확률의 단어 (limitation : Nnnn -> Anna)\n",
    "        #print(short_word)\n",
    "    return short_word\n",
    "\n",
    "#---------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokens =[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------------\n",
    "\n",
    "#input : chunk\n",
    "#output : chunk without speaker \n",
    "#ex : \"Narrator : Once upon a time,,...\" -> \"Once upon a time...\"\n",
    "def parse_lv1(chunk):\n",
    "    newchunk = ''.join(chunk.split(':')[1:])\n",
    "    return newchunk\n",
    "#input : chunk without speaker \n",
    "#output : chunk without speker/지시문 \n",
    "#ex : \"[seriously] Once upon a time,,...\" -> \"Once upon a time...\"\n",
    "regex = re.compile(r'\\[.+\\]')\n",
    "def parse_lv3(chunk):\n",
    "    return regex.sub('',chunk)\n",
    "#input : chunk without speker/지시문 \n",
    "#output : chuck without speark/지시문 r'-*'\n",
    "#ex : \"H--i\" -> \"Hi\"\n",
    "def parse_lv4(chunk):\n",
    "    tokens = []\n",
    "    for token in chunk.split():\n",
    "        if '-' in token:\n",
    "            if token in words.words():# mother-in-law -> should be accepted\n",
    "                newtoken = [token]\n",
    "            elif '-' in token: #else case\n",
    "                newtoken = token.replace('-',' ').split() # basically split the '-' \n",
    "                if ''.join(newtoken) in words.words():    # Ex-cuseme -> excusme case\n",
    "                    \n",
    "                    newtoken = [''.join(newtoken)]\n",
    "        else:\n",
    "            newtoken = [token]\n",
    "        tokens+=newtoken\n",
    "    return ' '.join(tokens)\n",
    "#------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input : string\n",
    "#output : boolean\n",
    "#description : check whether string contains alphabet.\n",
    "def alphabet(s):\n",
    "    for c in s:\n",
    "        if (c>='0' and c<='9'):    \n",
    "            return False\n",
    "    return True\n",
    "\n",
    "#input : string\n",
    "#output : boolen\n",
    "#description : check whether string's first letter is capital letter.\n",
    "def checkfirst(s):\n",
    "    return (s[0]==s[0].upper() and alphabet(s))\n",
    "\n",
    "#input : list of 덩어리\n",
    "#output : list of 단어\n",
    "#description : 겹치는 단어 제거\n",
    "def remove_prop(lst):\n",
    "    result=[]\n",
    "    prop=[]\n",
    "    for e in lst:\n",
    "        delimiters = \".\", \"!\", \"?\", \"\\n\"\n",
    "        regexPattern = '|'.join(map(re.escape, delimiters))\n",
    "        tmp = re.split(regexPattern, e)\n",
    "        tmp = list(filter(None, tmp))\n",
    "        \n",
    "        for t in tmp:\n",
    "            deli = \",\", \" \"\n",
    "            reg = \"|\".join(map(re.escape, deli))\n",
    "            ss=re.split(reg,t.strip())\n",
    "            ss=list(filter(None, ss))\n",
    "            #ss=t.strip().split()\n",
    "            result=result+ss\n",
    "            S=ss[1:]\n",
    "            for s in S:\n",
    "                if(checkfirst(s) and (not s in prop)):\n",
    "                    prop.append(s)\n",
    "\n",
    "    result = list(filter(lambda a: not a in prop, result))\n",
    "    return result\n",
    "\n",
    "\n",
    "# result=remove_prop(lst)\n",
    "# print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input : filepath,filename,field; ex) data/{field}/{filename} -> data/mylittlepony/mlp_001.txt\n",
    "#output : tokens = [word,word,word,...,word]\n",
    "def parse(filepath,filename,field):\n",
    "    #regex = re.compile(r'\\[.+\\]')\n",
    "    chunks =[]\n",
    "    with open(filepath,'r') as f:\n",
    "        \n",
    "        for chunk in f: # 덩어리 기준\n",
    "            chunk = chunk.strip()\n",
    "            #1 : 지시문, 화자는 제외\n",
    "            #1-1 화자 제거\n",
    "            chunk = parse_lv1(chunk)\n",
    "            #1-2 지시문 제거\n",
    "            chunk = parse_lv3(chunk)\n",
    "            #2 -와 -- 제거\n",
    "            chunk = parse_lv4(chunk)\n",
    "            #3 lengthenword correcting\n",
    "            #correct_lengthen(word)\n",
    "            newchunk = []\n",
    "            for word in chunk.split():\n",
    "                newchunk.append(correct_lengthen(word))\n",
    "            newchunk = ' '.join(newchunk)\n",
    "            #remove proper noun\n",
    "            chunks.append(newchunk)\n",
    "            \n",
    "        #print(newchunk)\n",
    "        #4 remove prop\n",
    "        wordset = remove_prop(chunks)\n",
    "    return wordset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tk = parse(filepath,filename,field)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    " \n",
    "filtered_sentence = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('doing.. ', 1)\n",
      "('doing.. ', 2)\n",
      "('doing.. ', 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bagdongmin/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:22: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/Users/bagdongmin/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:26: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('doing.. ', 4)\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "#pony processing\n",
    "\n",
    "\n",
    "field = 'mylittlepony'\n",
    "fnumber = 5\n",
    "for i in range(fnumber):\n",
    "    filename = \"mlp_{idx}.txt\".format(idx='{0:03}'.format(i+1))\n",
    "    filepath = 'data/{field}/{name}'.format(field=field,name=filename)\n",
    "    data += parse(filepath,filename,field)\n",
    "    if i%10==0:\n",
    "        print(\"doing.. \",i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stopword removing\n",
    "data = [w for w in data if not w in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 28)\n"
     ]
    }
   ],
   "source": [
    "n = len(data)\n",
    "n_file = (n//200) +1\n",
    "offset = n - 200*n_file+200\n",
    "print(offset, n_file)\n",
    "\n",
    "for i in range(n_file-1):\n",
    "    filename = \"parsed_mlp_{idx}.txt\".format(idx='{0:03}'.format(i+1))\n",
    "    filepath = 'data/{field}/{name}'.format(field=field,name=filename)\n",
    "    newdata = data[i*200:(i+1)*200]\n",
    "    with open(filepath,'w') as f:\n",
    "        f.write(' '.join(newdata))\n",
    "        f.write('\\n')\n",
    "filename = \"parsed_mlp_{idx}.txt\".format(idx='{0:03}'.format(n_file+1))\n",
    "filepath = 'data/{field}/{name}'.format(field=field,name=filename)\n",
    "newdata = data[n_file*200:]\n",
    "with open(filepath,'w') as f:\n",
    "    with open(filepath,'w') as f:\n",
    "        f.write(' '.join(newdata))\n",
    "        f.write('\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stopwords removing using NLTK I, Mr. / I'll I've  /  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
