{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bagdongmin/anaconda2/lib/python2.7/site-packages/cffi/model.py:525: UserWarning: 'point_conversion_form_t' has no values explicitly defined; guessing that it is equivalent to 'unsigned int'\n",
      "  % self._get_c_name())\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import re\n",
    "from pattern.en import suggest\n",
    "filename = \"mlp_001.txt\"\n",
    "field = 'mylittlepony'\n",
    "filepath = 'data/{field}/{name}'.format(field=field,name=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#----------by DM-----------\n",
    "\n",
    "#input : word, output : word\n",
    "\n",
    "#---------------------\n",
    "\n",
    "\n",
    "\n",
    "#...제외 3개이상 알파벳 반복 -> 2개\n",
    "def three2two(word):\n",
    "    pattern = re.compile(r'([^\\.]+)\\1{2,}')\n",
    "    return pattern.sub(r'\\1\\1', word)\n",
    "\n",
    "# word -> word ('helllloooo' -> 'hello')\n",
    "def correct_lengthen(word):\n",
    "    short_word = three2two(word)\n",
    "    if short_word != word:\n",
    "        p_word = suggest(short_word) #(가장 유사한 단어, 확률) tuples\n",
    "        short_word = p_word[0][0] #제일 높은 확률의 단어 (limitation : Nnnn -> Anna)\n",
    "        #print(short_word)\n",
    "    return short_word\n",
    "\n",
    "#---------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokens =[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------------\n",
    "\n",
    "#input : chunk\n",
    "#output : chunk without speaker \n",
    "#ex : \"Narrator : Once upon a time,,...\" -> \"Once upon a time...\"\n",
    "def parse_lv1(chunk):\n",
    "    newchunk = ''.join(chunk.split(':')[1:])\n",
    "    return newchunk\n",
    "#input : chunk without speaker \n",
    "#output : chunk without speker/지시문 \n",
    "#ex : \"[seriously] Once upon a time,,...\" -> \"Once upon a time...\"\n",
    "regex = re.compile(r'\\[.+\\]')\n",
    "#regex = re.compile(r'\\(.*?\\)') #(seriously)제거용\n",
    "def parse_lv3(chunk):\n",
    "    return regex.sub('',chunk)\n",
    "#input : chunk without speker/지시문 \n",
    "#output : chuck without speark/지시문 r'-*'\n",
    "#ex : \"H--i\" -> \"Hi\"\n",
    "def parse_lv4(chunk):\n",
    "    tokens = []\n",
    "    for token in chunk.split():\n",
    "        if '-' in token:\n",
    "            if token in words.words():# mother-in-law -> should be accepted\n",
    "                newtoken = [token]\n",
    "            elif '-' in token: #else case\n",
    "                newtoken = token.replace('-',' ').split() # basically split the '-' \n",
    "                if ''.join(newtoken) in words.words():    # Ex-cuseme -> excusme case\n",
    "                    newtoken = [''.join(newtoken)]\n",
    "        else:\n",
    "            newtoken = [token]\n",
    "        tokens+=newtoken\n",
    "    return ' '.join(tokens)\n",
    "#------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input : string\n",
    "#output : boolean\n",
    "#description : check whether string contains alphabet.\n",
    "def alphabet(s):\n",
    "    for c in s:\n",
    "        if (c>='0' and c<='9'):    \n",
    "            return False\n",
    "    return True\n",
    "\n",
    "#input : string\n",
    "#output : boolen\n",
    "#description : check whether string's first letter is capital letter.\n",
    "def checkfirst(s):\n",
    "    return (s[0]==s[0].upper() and alphabet(s))\n",
    "\n",
    "#input : list of 덩어리\n",
    "#output : list of 단어\n",
    "#description : 겹치는 단어 제거\n",
    "def remove_prop(lst):\n",
    "    result=[]\n",
    "    prop=[]\n",
    "    for e in lst:\n",
    "        delimiters = \".\", \"!\", \"?\", \"\\n\"\n",
    "        regexPattern = '|'.join(map(re.escape, delimiters))\n",
    "        tmp = re.split(regexPattern, e)\n",
    "        tmp = list(filter(None, tmp))\n",
    "        \n",
    "        for t in tmp:\n",
    "            deli = \",\", \" \"\n",
    "            reg = \"|\".join(map(re.escape, deli))\n",
    "            ss=re.split(reg,t.strip())\n",
    "            ss=list(filter(None, ss))\n",
    "            #ss=t.strip().split()\n",
    "            result=result+ss\n",
    "            S=ss[1:]\n",
    "            for s in S:\n",
    "                if(checkfirst(s) and (not s in prop)):\n",
    "                    prop.append(s)\n",
    "\n",
    "    result = list(filter(lambda a: not a in prop, result))\n",
    "    return result\n",
    "\n",
    "\n",
    "# result=remove_prop(lst)\n",
    "# print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input : filepath,filename,field; ex) data/{field}/{filename} -> data/mylittlepony/mlp_001.txt\n",
    "#output : tokens = [word,word,word,...,word]\n",
    "def parse(filepath,filename,field):\n",
    "    #regex = re.compile(r'\\[.+\\]')\n",
    "    chunks =[]\n",
    "    with open(filepath,'r') as f:\n",
    "        \n",
    "        for chunk in f: # 덩어리 기준\n",
    "            chunk = chunk.strip()\n",
    "            #1 : 지시문, 화자는 제외\n",
    "            #1-1 화자 제거\n",
    "            #chunk = parse_lv1(chunk)\n",
    "            #1-2 지시문 제거\n",
    "            chunk = parse_lv3(chunk)\n",
    "            #2 -와 -- 제거\n",
    "            chunk = parse_lv4(chunk)\n",
    "            #3 lengthenword correcting\n",
    "            #correct_lengthen(word)\n",
    "            newchunk = []\n",
    "            for word in chunk.split():\n",
    "                newchunk.append(correct_lengthen(word))\n",
    "            newchunk = ' '.join(newchunk)\n",
    "            #remove proper noun\n",
    "            chunks.append(newchunk)\n",
    "            \n",
    "        #print(newchunk)\n",
    "        #4 remove prop\n",
    "        wordset = remove_prop(chunks)\n",
    "    return wordset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tk = parse(filepath,filename,field)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    " \n",
    "filtered_sentence = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('doing.. ', 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bagdongmin/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:22: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/Users/bagdongmin/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:26: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('doing.. ', 10)\n",
      "('doing.. ', 20)\n",
      "('doing.. ', 30)\n",
      "('doing.. ', 40)\n",
      "('doing.. ', 50)\n",
      "('doing.. ', 60)\n",
      "('doing.. ', 70)\n",
      "('doing.. ', 80)\n",
      "('doing.. ', 90)\n",
      "('doing.. ', 100)\n",
      "('doing.. ', 110)\n",
      "('doing.. ', 120)\n",
      "('doing.. ', 130)\n",
      "('doing.. ', 140)\n",
      "('doing.. ', 150)\n",
      "('doing.. ', 160)\n",
      "('doing.. ', 170)\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "#pony processing\n",
    "\n",
    "field = 'mylittlepony'\n",
    "fnumber = 176\n",
    "for i in range(fnumber):\n",
    "    filename = \"mlp_{idx}.txt\".format(idx='{0:03}'.format(i+1))\n",
    "    filepath = 'data/{field}/{name}'.format(field=field,name=filename)\n",
    "    data += parse(filepath,filename,field)\n",
    "    if i%10==0:\n",
    "        print(\"doing.. \",i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('doing.. ', 0)\n",
      "('doing.. ', 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bagdongmin/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:22: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/Users/bagdongmin/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:26: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('doing.. ', 20)\n",
      "('doing.. ', 30)\n",
      "('doing.. ', 40)\n",
      "('doing.. ', 50)\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "#fairytail processing\n",
    "\n",
    "field = 'fairytail'\n",
    "fnumber = 55\n",
    "for i in range(fnumber):\n",
    "    filename = \"_{idx}.txt\".format(idx=i+1)\n",
    "    filepath = 'data/{field}/{name}'.format(field=field,name=filename)\n",
    "    data += parse(filepath,filename,field)\n",
    "    if i%10==0:\n",
    "        print(\"doing.. \",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('doing.. ', 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bagdongmin/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:22: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/Users/bagdongmin/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:26: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('doing.. ', 10)\n",
      "('doing.. ', 20)\n",
      "('doing.. ', 30)\n",
      "('doing.. ', 40)\n",
      "('doing.. ', 50)\n",
      "('doing.. ', 60)\n",
      "('doing.. ', 70)\n",
      "('doing.. ', 80)\n",
      "('doing.. ', 90)\n",
      "('doing.. ', 100)\n",
      "('doing.. ', 110)\n",
      "('doing.. ', 120)\n",
      "('doing.. ', 130)\n",
      "('doing.. ', 140)\n",
      "('doing.. ', 150)\n",
      "('doing.. ', 160)\n",
      "('doing.. ', 170)\n",
      "('doing.. ', 180)\n",
      "('doing.. ', 190)\n",
      "('doing.. ', 200)\n",
      "('doing.. ', 210)\n",
      "('doing.. ', 220)\n",
      "('doing.. ', 230)\n",
      "('doing.. ', 240)\n",
      "('doing.. ', 250)\n",
      "('doing.. ', 260)\n",
      "('doing.. ', 270)\n",
      "('doing.. ', 280)\n",
      "('doing.. ', 290)\n",
      "('doing.. ', 300)\n",
      "('doing.. ', 310)\n",
      "('doing.. ', 320)\n",
      "('doing.. ', 330)\n",
      "('doing.. ', 340)\n",
      "('doing.. ', 350)\n",
      "('doing.. ', 360)\n",
      "('doing.. ', 370)\n",
      "('doing.. ', 380)\n",
      "('doing.. ', 390)\n",
      "('doing.. ', 400)\n",
      "('doing.. ', 410)\n",
      "('doing.. ', 420)\n",
      "('doing.. ', 430)\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "#spongebob processing\n",
    "\n",
    "field = 'spongebob'\n",
    "fnumber = 439\n",
    "for i in range(fnumber):\n",
    "    filename = \"_{idx}.txt\".format(idx=i+1)\n",
    "    filepath = 'data/{field}/{name}'.format(field=field,name=filename)\n",
    "    data += parse(filepath,filename,field)\n",
    "    if i%10==0:\n",
    "        print(\"doing.. \",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('doing.. ', 0)\n",
      "('doing.. ', 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bagdongmin/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:23: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/Users/bagdongmin/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:27: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('doing.. ', 20)\n",
      "('doing.. ', 30)\n",
      "('doing.. ', 40)\n",
      "('doing.. ', 50)\n",
      "('doing.. ', 60)\n",
      "('doing.. ', 70)\n",
      "('doing.. ', 80)\n",
      "('doing.. ', 90)\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "#shameless processing\n",
    "\n",
    "field = 'shameless'\n",
    "fnumber = 96\n",
    "for i in range(fnumber):\n",
    "    filename = \"{idx}.txt\".format(idx=i)\n",
    "    filepath = 'data/{field}/{name}'.format(field=field,name=filename)\n",
    "    data += parse(filepath,filename,field)\n",
    "    if i%10==0:\n",
    "        print(\"doing.. \",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('doing.. ', 0)\n",
      "('doing.. ', 10)\n",
      "('doing.. ', 20)\n",
      "('doing.. ', 30)\n",
      "('doing.. ', 40)\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "#GameOfThrone processing\n",
    "\n",
    "field = 'GameOfThrone'\n",
    "fnumber = 50\n",
    "for i in range(fnumber):\n",
    "    filename = \"_{idx}.txt\".format(idx=i+1)\n",
    "    filepath = 'data/{field}/{name}'.format(field=field,name=filename)\n",
    "    data += parse(filepath,filename,field)\n",
    "    if i%10==0:\n",
    "        print(\"doing.. \",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bagdongmin/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:23: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/Users/bagdongmin/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:27: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('doing.. ', 0)\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "#GameOfThrone processing\n",
    "\n",
    "field = 'BadWords'\n",
    "fnumber = 1\n",
    "for i in range(fnumber):\n",
    "    filename = \"{idx}.txt\".format(idx=i+1)\n",
    "    filepath = 'data/{field}/{name}'.format(field=field,name=filename)\n",
    "    data += parse(filepath,filename,field)\n",
    "    if i%10==0:\n",
    "        print(\"doing.. \",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not',\n",
       " 'that',\n",
       " 'good',\n",
       " 'at',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'stuff',\n",
       " 'Especially',\n",
       " 'thinking']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "s = 's'\n",
    "print(\"asdfasdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "off“ is not unicode\n",
      "off“ is not unicode\n",
      "“ is not unicode\n"
     ]
    }
   ],
   "source": [
    "#stemming ' ,'\n",
    "stringdata = ' '.join(data)\n",
    "data = word_tokenize(stringdata)\n",
    "from nltk.stem import *\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "#data = [stemmer.stem(str(unicode(w))) for w in data]\n",
    "\n",
    "new_data = []\n",
    "for w in data:\n",
    "    try:\n",
    "        new_data.append(stemmer.stem(str(unicode(w))))\n",
    "    except UnicodeDecodeError:\n",
    "        print('{} is not unicode'.format(w))\n",
    "\n",
    "#print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'not',\n",
       " u'that',\n",
       " u'good',\n",
       " 'at',\n",
       " 'a',\n",
       " u'lot',\n",
       " 'of',\n",
       " u'stuff',\n",
       " u'especi',\n",
       " u'think']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stopword removing\n",
    "data = [w for w in new_data if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BadWords 1.txt\n"
     ]
    }
   ],
   "source": [
    "print field, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139, 23)\n"
     ]
    }
   ],
   "source": [
    "n = len(data)\n",
    "n_file = (n//200) +1\n",
    "offset = n - 200*n_file+200\n",
    "print(offset, n_file)\n",
    "\n",
    "for i in range(n_file-1):\n",
    "    #filename = \"parsed{idx}.txt\".format(idx='{0:03}'.format(i+1))\n",
    "    filename = \"parsed{idx}.txt\".format(idx=i+1)\n",
    "    filepath = 'data_parsed/{field}/{name}'.format(field=field,name=filename)\n",
    "    newdata = data[i*200:(i+1)*200]\n",
    "    with open(filepath,'w') as f:\n",
    "        f.write(' '.join(newdata))\n",
    "        f.write('\\n')\n",
    "        \n",
    "filename = \"parsed{idx}.txt\".format(idx=n_file)\n",
    "filepath = 'data_parsed/{field}/{name}'.format(field=field,name=filename)\n",
    "newdata = data[(n_file-1)*200:]\n",
    "#print(len(newdata))\n",
    "with open(filepath,'w') as f:\n",
    "    with open(filepath,'w') as f:\n",
    "        f.write(' '.join(newdata))\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stopwords removing using NLTK I, Mr. / I'll I've  /  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
