{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import sklearn\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "\n",
    "def search(dirname):\n",
    "    filenames = os.listdir(dirname)\n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(dirname, filename)\n",
    "        print (full_filename)\n",
    "\n",
    "def train_nb(training_documents):\n",
    "    #input : list of string.\n",
    "\n",
    "    #output. classifier\n",
    "    return MultinomialNB().fit(training_documents[0],training_documents[1])\n",
    "\n",
    "    \n",
    "\n",
    "def classify_nb(classifier_data, document):\n",
    "    #input : classifier, list of string\n",
    "    #output : prediction\n",
    "    global m_vec\n",
    "    d_counts = m_vec.transform(document)\n",
    "    d_tfidf = tfidf_trans.transform(d_counts)\n",
    "\n",
    "    prediction = classifier_data.predict(d_tfidf)\n",
    "    \n",
    "    return (document, prediction) \n",
    "\n",
    "def pretty_print(result):\n",
    "    for text, posneg in result:\n",
    "        print('%s => %s' %(str(text, 'utf-8'), m_train.target_names[posneg]))\n",
    "    return\n",
    "\n",
    "def result_print(result):\n",
    "    res=[]\n",
    "    for text, posneg in result:\n",
    "        res.append(m_train.target_names[posneg])\n",
    "    print(res)\n",
    "    return\n",
    "\n",
    "def main():\n",
    "#     train_dir = input(\"Enter your data directory name (default : txt_sentoken)\\n>>\")\n",
    "    m_dir = 'data_final_no'\n",
    "#     if(train_dir == \"\"):\n",
    "    train_dir = m_dir\n",
    "    global m_train, m_vec, tfidf_trans\n",
    "    m_train = load_files(train_dir)\n",
    "    X,x,Y,y = train_test_split(m_train.data,m_train.target,test_size = 0.2)\n",
    "    #X,Y ==> train\n",
    "    #x,y ==> test\n",
    "\n",
    "    m_vec = CountVectorizer(min_df = 1, tokenizer=nltk.word_tokenize)\n",
    "    train_counts = m_vec.fit_transform(X)\n",
    "    test_counts = m_vec.transform(x)\n",
    "    tfidf_trans = TfidfTransformer()\n",
    "    x_train = tfidf_trans.fit_transform(train_counts)\n",
    "    x_test = tfidf_trans.transform(test_counts)\n",
    "    y_train = Y\n",
    "    y_test = y\n",
    "\n",
    "    classifier = train_nb((x_train, y_train))\n",
    "\n",
    "    doc, y_pred = classify_nb(classifier, x)\n",
    "    result = zip(doc, y_pred)\n",
    "    #pretty_print(result)\n",
    "    target_names = m_train.target_names\n",
    "\n",
    "    report = sklearn.metrics.classification_report(y_test, y_pred, labels=None, target_names=target_names, sample_weight=None, digits=2)\n",
    "#     print(report)\n",
    "    return classifier,X\n",
    "\n",
    "# main()\n",
    "classifier,X = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "# example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "# word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "# filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    " \n",
    "# filtered_sentence = []\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "#data = [stemmer.stem(str(unicode(w))) for w in data]\n",
    "\n",
    "# new_data = []\n",
    "# for w in data:\n",
    "#     try:\n",
    "#         new_data.append(stemmer.stem(str(unicode(w))))\n",
    "def create_input(string_input):\n",
    "    string_input = ' '.join([w for w in string_input.split() if w not in stop_words])\n",
    "    data = word_tokenize(string_input)\n",
    "    new_data = []\n",
    "    for w in data:\n",
    "        new_data.append(stemmer.stem(str(w)))\n",
    "    \n",
    "    return [' '.join(new_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['everi person asshol'] [0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "doc,y_pred = classify_nb(classifier,create_input('every person should asshole'))\n",
    "print(doc,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey apple this is boobs'] [1]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
